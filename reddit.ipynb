{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "import glob\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import chain\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "urls = ['https://www.reddit.com/r/london/comments/15rqct3/ideas_for_london_activities_for_my_dad/',\n",
    "'https://www.reddit.com/r/london/comments/rvordl/places_you_actively_avoid_in_london/',\n",
    " 'https://www.reddit.com/r/london/comments/pdsnpm/what_do_people_actually_do_in_london_that_isnt/',\n",
    " 'https://www.reddit.com/r/TravelHacks/comments/11hfoix/going_to_london_england_for_a_week_any/',\n",
    "'https://www.reddit.com/r/travel/comments/14sc0t3/london_definitely_exceeded_my_expectations/'\n",
    "'https://www.reddit.com/r/finedining/comments/13yc8vl/what_is_your_favorite_restaurant_in_london_in_2023/'\n",
    " 'https://www.reddit.com/r/TravelHacks/comments/14ae4af/what_are_some_interesting_things_to_do_in_london/',\n",
    "  'https://www.reddit.com/r/londonontario/comments/wm01m6/what_are_the_top_5_things_to_do_in_london/'\n",
    "       'https://www.reddit.com/r/london/comments/1855c15/what_is_getting_better_in_london/'\n",
    "]\n",
    "\n",
    "the_dates = []\n",
    "the_contents = []\n",
    "def scrape(url):\n",
    "    service = Service(executable_path=r'path/to/chromedriver.exe')\n",
    "    options = Options()\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    #options.add_argument(\"--headless\")\n",
    "    #options.add_argument(\"--no-sandbox\")\n",
    "    #options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "\n",
    "    driver = webdriver.Chrome(service=service,options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.XPATH,\"//span/span[@class='flex items-center gap-xs']\")))\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "    elements = driver.find_elements(By.XPATH, \"//button[@class='text-tone-2 text-12 no-underline hover:underline px-xs py-xs flex ml-[3px] xs:ml-0 !bg-transparent !border-0']\")\n",
    "    #elements = driver.find_elements(By.LINK_TEXT, \"more replies\")\n",
    "  \n",
    "    time.sleep(30)\n",
    "    # Click each element\n",
    "    for element in elements:\n",
    "        try:\n",
    "            element.click()\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking element: {e}\")\n",
    "            # Continue processing other elements\n",
    "            \n",
    "    time.sleep(30)\n",
    "    # Click each element\n",
    "    for element in elements:\n",
    "        try:\n",
    "            element.click()\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking element: {e}\")\n",
    "            # Continue processing other elements\n",
    "            \n",
    "\n",
    "\n",
    "    time.sleep(10)\n",
    "    print('one')\n",
    "    dates =driver.find_elements(By.XPATH,\"//faceplate-timeago[@class='text-neutral-content-weak text-12']/time\")\n",
    "    date = [con.get_attribute('title') for con in dates]\n",
    "    print(len(dates))\n",
    "    the_dates.append(date)\n",
    "\n",
    "    print('two')\n",
    "    contents =driver.find_elements(By.CLASS_NAME, \"py-0.mx-sm.inline-block.max-w-full\")\n",
    "    content = [con.get_attribute('innerText') for con in contents]\n",
    "    the_contents.append(content)\n",
    "    #driver.quit()\n",
    "\n",
    "    \n",
    "for i in urls:\n",
    "    scrape(i)\n",
    "    print(i)\n",
    "    print(len(the_dates))\n",
    "    print(len(the_contents))\n",
    "\n",
    "import itertools\n",
    "da = list(itertools.chain(*the_dates))\n",
    "co = list(itertools.chain(*the_contents))\n",
    "data = {'date_posted': da, 'content': co}\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.transpose()\n",
    "\n",
    "df.to_csv('london.csv')\n",
    "all_files = glob.glob('*.csv')\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Read each CSV file and append it to the DataFrame\n",
    "for file in all_files:\n",
    "    data = pd.read_csv(file)\n",
    "    df = df.append(data, ignore_index=True)\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('london_reddit_merged_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
